<!DOCTYPE HTML PUBLIC>
<!--
    Aerial by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="google-site-verification" content="uJ3L5h2rK92y4dNJjPaCjQaw4Dys-zEk-1M2ESaYMeM" />
  <!-- <title>Franco Zivcovich</title> -->
  <link rel="stylesheet" href="styles.css">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
  <title>Franco Zivcovich</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="Franco Zivcovich">
  <meta property="og:locale" content="en_US">
  <meta name="description" content="Franco Zivcovich">
  <meta property="og:description" content="Franco Zivcovich">
  <link rel="canonical" href="https://francozivcovich.github.io/">
  <meta property="og:url" content="https://francozivcovich.github.io/">
  <meta property="og:site_name" content="Franco Zivcovich">

  <link rel="shortcut icon" href="imgs/favicon.ico" type="image/x-icon">
  <link rel="icon" href="imgs/favicon.ico" type="image/x-icon">

  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>




  <div class="row">
    <div class="column left">

      <!-- <br>
      <br>
      <br>
      <br> -->
      <figure>
        <img src="imgs/nice_napoli.jpg" width=108% height: auto>
        <figcaption align="right"><p style="color:#A3A2A2";><small><small>Posillipo - Gulf of Naples, Italy, 2021.</small></small></p></figcaption>
      </figure>

      <br>

      <h1>Welcome to my homepage!</h1>

      <br>
      <div>
      <p>
        Monday to Friday, before 6pm, I am a Ph.D. in Mathematics operating as
        research scientist in the field of Numerical Analysis
        and High Performance Scientific Computing.
      <br>
        The rest of the time I enjoy riding my bike, eating, and cooking. I am
        very passionate about mountains and outdoor activities such as camping
        and barbecueing in the wild.
        <!-- I sometimes linger googling myself like <i>damn this mf is cool as hell</i>. -->
      </p>
      </div>

      <br>

      <div>
      <p>
        If you want to know more about my
        <a href="#current-position">current position</a>,
        <a href="#research-interests">research interests</a>,
        and
        <a href="#passion-projects ">passion projects</a>
        you're in the right place!
        <br>
        You want to read more details about my career? Check out my
        <a href="pdf-files/francozivcovich-cv.pdf" target="_blank">Curriculum Vitae</a></li>
        or
        <a href="https://scholar.google.com/citations?user=oHN16zwAAAAJ">Google Scholar</a> page.
      </p>
      </div>

      <br>

      <div>
        <p>
          <!-- If instead you feel like complaining about something wrong I did, feel welcome to drop me an email at -->
          If instead you are one of my students and wish to ask me some questions,
          you are welcome to drop me an email here:
          <br>
          <br>
          <marquee direction="right" behavior="alternate" Scrollamount=16>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <span style="color: #0008ff">franco[dot]zivcovich[at]gmail[dot]com</span>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </marquee></li>
        </p>
      </div>


      <!-- Here something you might be looking for:
      <ul>
        <li> my detailed <a href="pdf-files/francozivcovich-cv.pdf" target="_blank">Curriculum Vitae</a></li>
        <li> my <a href="https://scholar.google.com/citations?user=oHN16zwAAAAJ">Google Scholar</a> page</li>
          <marquee direction="right" behavior="alternate" Scrollamount=16>
            <li><small>if instead I did something wrong, here's my email: <span style="color: #0008ff">franco.zivcovich[at]gmail[dot]com</span></small>
          </marquee></li>
        </ul>
      </p> -->


    </div>

    <div class="column right">
      <h0>Franco Zivcovich</h0>
      <h3>Mathematician, pizza expert.</h3>
      <h6 style="color:#A3A2A2";>(site under construction - last update: January 2023)</h6>



      <h2 id="current-position">Current position</h2>
        <p>
        I currently work in Nice, France, as research scientist for <a href="http://neurodec.ai/">Neurodec</a>.

        My colleagues and I are developing our <i>Myoelectric Digital Twin</i>,
        i.e., a virtual clone of your arm where the quasi-static Maxwell
        equations ruling the physics inside of it are precisely reproduced.
        <br>
        <i>Why that?</i> You may ask. Well, the force exerted by a muscle
        depends on the number of motor units (neurons commanding bunches of
        muscle fibers) activation and the rates at which they discharge action
        potentials.
        </p>

        <figure>
        <img src="imgs/neurodec/fiber_properties.gif" width=100% height: auto>
        <figcaption>
        <small><small><center><p>
        The action potentials are "wrinkles" in the base-line electric potential
        that propagates from neuromuscular junctions along muscle fibers until
        they hit the tendons.
        </p></center></small></small>
        </figcaption>
        </figure>


        <p>
        This process produces an electromagnetic 'footprint' (EMG signals) that
        can be recorded by skin electrodes and use to train a computer to
        associate movements and gestures to such 'footprints'.
        </p>


        <figure>
        <img src="imgs/neurodec/skin_electrodes_on_me.jpeg" width=100% height: auto>
        <figcaption>
        <small><small><center><p>
        Sometimes my job requires me being a guinea pig :C
        </p></center></small></small>
        </figcaption>
        </figure>

        <p>
        However, the acquisition of real EMG data is time-consuming and expensive. It
        requires expert knowledge and is error-prone even in the best circumstances.
        Moreover, the produced dataset has limited variability, is highly
        specific and it is only partially labeled, at best.
        </p>

        <figure>
        <img src="imgs/neurodec/emg_robot-2.png" width=32% height: auto>&nbsp&nbsp&nbsp&nbsp<img src="imgs/neurodec/emg_hmi.png" width=30% height: auto>&nbsp&nbsp&nbsp&nbsp<img src="imgs/neurodec/emg_human.png" width=30% height: auto>
        <figcaption>
        <small><small><center><p>
        Applications include Robotic Control, Metaverse, Medicine and Sport
        (images credits to <a href="http://neurodec.ai/"><small>Neurodec</small></a>)
        </p></center></small></small>
        </figcaption>
        </figure>
        <p>
        Here at Neurodec, we develope the
        <a href="http://neurodec.ai/mdt/documentation/index.html">MDT</a>
        software, capable of simulating arbitrary large datasets of ultra-realistic
        synthetic EMG signals.
        The simulation is very fast, the data is extremely precise and perfectly
        labeled, making it ideal for training industrial AI-based algorithms.
        </p>




      <h2 id="research-interests">Research interests</h2>

      <p>
      My research interests include a host of topics from the <i>High Performance
      Scientific Computing</i> domain.

      In this section I give a quick overview of some of them.
      </p>
      <p>
      Table of contents:
      <ul>
        <li><a href="#finite-elements">click to go to</a>: Low-Tech High-Efficiency CPU/GPU Finite Elements implementations</li>
        <li><a href="#exp-integrators">click to go to</a>: Design of integrator-solutor pairs in Exponential Integration of stiff PDEs</li>
        <li><a href="#optimal-control">click to go to</a>: Numerical methods for Multiscale and Optimal Control problems</li>
        <li><a href="#num-lin-algebra">click to go to</a>: Numerical Linear Algebra and Arbitrary Precision arithmetic</li>
      </ul>
      </p>

      <h3 id="finite-elements">Low-Tech High-Efficiency CPU/GPU Finite Elements implementations</h3>
      <p>
        One day, tired of the unnerving intricacy of the state-of-the-arts FE
        implementations, I created my own rudimental FE environment.
        What I expected to be no more than a handy inquiry tool, turned out
        being quite a <i>slick and highly-efficient gadget</i>.
      </p>

        <figure>
        <img src="imgs/research_interests/finite-elements/FEM_p0_003.png" width=32% height: auto>
        <img src="imgs/research_interests/finite-elements/FEM_p0_002.png" width=32% height: auto>
        <img src="imgs/research_interests/finite-elements/FEM_p0_001.png" width=32% height: auto>
        <figcaption><small><small><p>
        Piecewise constant component of Mixed-Elements solution (lowest order
        Raviart-Thomas Elements coupled with piecewise constant FE) to Poisson equation.
        </p></small></small></figcaption>
        </figure>
      <p>
        Now, I aim to assemble, <i>in less than a thousand</i> MATLAB/Python
        lines, the richest FE software possible, and to make it faster than
        the best-established <i>compiled</i> FE machineries.
        Will I be successful? Can't say yet. As now, I only got one of the
        fastest generator ever of colorful pictures.
      </p>





      <h3 id="exp-integrators">Design of integrator-solutor pairs in Exponential Integration of stiff PDEs</h3>

      <p>
      When simulating dynamics, more accuracy means more effort. This is especially
      true when the underlying differential equations are <b>stiff</b>.
      Stiff systems are characterized by a wide range of time scales in their
      evolution.
      Let's write our stiff system of differential equations like this:
      </p>
      <figure>
        <center>
      <img src="imgs/research_interests/exp-integrators/stiffode.png" width=50% height: auto>
        </center>
      </figure>
      <p>
        so that the stiffness is concentrated in the linearity <b>A</b>.
        Usually the matrix <b>A</b> comes from the space discretization of
        differential operators from Partial Differential equations, it is
        therefore also extremely <b>stiff, large, and sparse</b>.

        Exponential integrators are usually derived from the Duhamel formula
      </p>
      <figure>
        <center>
      <img src="imgs/research_interests/exp-integrators/duhamel.png" width=70% height: auto>
        </center>
      </figure>
      <p>
        where the linearity <b>A</b> is treated exactly. For this reason, they
        are especially suited for the integration of stiff systems of
        differential equations (see [HO10]). Each exponential-type method differs from the
        others for how it approximates the integral appearing in the formula
        above, usually by means of few linear combinations of phi-functions:
      </p>
      <figure>
        <center>
      <img src="imgs/research_interests/exp-integrators/lincombphifun.png" width=52% height: auto>
        </center>
      </figure>
      <p>
      But, are these linear combinations of phi-functions difficult to compute?
      Well, not really, we can obtain the combination above through the single,
      slightly larger, action of the matrix exponential:
      <figure>
        <center>
      <img src="imgs/research_interests/exp-integrators/task.png" width=9% height: auto>
        </center>
      </figure>
      <p>
      where
      </p>
      <figure>
        <center>
      <img src="imgs/research_interests/exp-integrators/whoAtilde.png" width=100% height: auto>
        </center>
      </figure>
      </p>

      <p>
        When dealing with such matrices you have to be careful to what you do:
      </p>
      <ul>
        <li>it's <b>okay</b> to multiply <b>A</b> by vectors</li>
        <li>it's <b>NOT okay</b> to form functions of <b>A</b></li>
      </ul>
      <p>
        In fact, say that <b>A</b> is the Finite Differences discretization of
        the two-dimensional Laplacian operator over a square 128x128 grid,
        we have that
      </p>
      <ul>
        <li>storing <b>A</b> requires about 2Mb of space</li>
        <li>storing the <b>exponential</b> of <b>A</b> requires about 4.3Gb(!!!) of space</li>
      </ul>
      <p>
        Hence, similarly to when solving linear systems you never actually
        compute the inverse of the lhs matrix, we do not form the exponential of
        <b>&Atilde;</b> but just its <i>action</i>.
      </p>

      <p>
        <small>
        <i>References:</i>
        <br>
        [HO10] M. Hochbruck, A. Ostermann, Exponential integrators, Acta Numer. 19 (2010) 209–286.
        </small>
      </p>


      <h4>The Kronecker's "pro-gamer" move</h4>
      <p>
      When an evolutionary PDE is numerically treated with the method of lines
      over domains that are Cartesian product of <i>d</i> intervals, <b>A</b> is
      a Kronecker sum, that is
      </p>
      <figure>
          <img src="imgs/research_interests/exp-integrators/kroneckerform.png" width=105% height: auto>
      </figure>
      <p>
      where the rounded x indicates the <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker product</a>.
      <br>
      Now, the cool thing about matrices in this form is that their exponential
      can be written as
      </p>
      <figure>
        <center>
          <img src="imgs/research_interests/exp-integrators/expkroneckernaive.png" width=55% height: auto>
        </center>
      </figure>
      <p>
      which is a rather stupid way to form it, as it implies to form a huge full
      matrix and then to multiply it into a vector.
      On the other hand, this is equivalent to
      </p>
      <figure>
        <center>
          <img src="imgs/research_interests/exp-integrators/expkroneckerclever.png" width=70% height: auto>
        </center>
      </figure>
      <p>
      where <b>U</b> is a <i>d</i>-dimensional tensor such that vec(<b>U</b>),
      i.e. stacking its columns one on the top of the other, equals v, and
      the strangely subscripted cross products are called &mu;-mode products.
      <br>
      The &mu;-mode product takes the the <i>d</i>-dimensional tensor <b>U</b>
      and the exponential of <b>A</b><sub>&mu;</sub>,
      and it does the following serie of operations to them:
      </p>
      <ul>
      <li>rotates  it to expose its &mu;th face;</li>
      <li>reshapes it so that it becomes a two-dimensional matrix with n<sub>&mu;</sub> rows;</li>
      <li>multiplies the exponential of <b>A</b><sub>&mu;</sub> into it;</li>
      <li>reshapes the result back into the original form of <b>U</b>.</li>
      </ul>
      <p>
      The above formula is nothing more than the <i>d</i>-dimensional
      generalization of the well-known two dimensional formula
      </p>
      <figure>
        <center>
          <img src="imgs/research_interests/exp-integrators/expkroneckerclever2D.png" width=40% height: auto>
        </center>
      </figure>
      <p>
      This procedure is madly efficient: the <i>d</i> matrix exponentials are
      usually rather small and can easily be formed once and for all while the
      &mu;-mode products rely on the
      <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">level 3 gemm BLAS</a>
      (Generalized Matrix Matrix product from the Basic Linear Algebra Subprograms).
      </p>

      <p>
      In particular, we shown in
      <a href="https://scholar.google.com/citations?view_op=view_citation&hl=it&user=oHN16zwAAAAJ&citation_for_view=oHN16zwAAAAJ:UeHWp8X0CEIC">[CCEOZ22]</a>
      that the performance on a GPU architeture can be so extreme that are close
      to the theoretical limit of the hardware.
      </p>

      <p>
      Now, to compute linear combinations of phi-function of matrix in Kronecker
      product is trickier since <b>&Atilde;</b> is not in Kronecker form even
      though <b>A</b> is.
      However, in
      <a href="https://arxiv.org/pdf/2210.07667.pdf">[CCZ23k]<a>
      we managed to tackle this issue using an alternative integral definition
      of these functions. Results are very promising and we are very active in
      this line of work.
      </p>

      <p>
        <small>
        <i>References:</i>
        <br>
        [CCEOZ22] M. Caliari, F. Cassini, L. Einkemmer, A. Ostermann, F. Zivcovich, A &mu;-mode integrator for solving evolution equations in Kronecker form, J. Comput. Phys. 455 (2022) 110989.
        <br>
        [CCZ22]   M. Caliari, F. Cassini, F. Zivcovich, A &mu;-mode BLAS approach for multidimensional tensor structured problems, Numer. Algorithms (2022).
        <br>
        [CCZ23k]  M. Caliari, F. Cassini, F. Zivcovich, A &mu;-mode approach for exponential integrators: actions of &phi;-functions of Kronecker sums, ArXiv (2023).
        </small>
      </p>




      <h4>The BAMPHI routine</h4>
      <p>
        To form linear combinations of the phi-functions when the matrix to be
        exponentiate is not in Kronecker form one has to settle for something
        more involved and expensive.
        A prominent way is represented by the so called Krylov-type approximations:
      </p>
      <figure>
        <center>
          <img src="imgs/research_interests/exp-integrators/krylovapprox.png" width=40% height: auto>
        </center>
      </figure>
      <p>
        where m << N, <b>V</b><sub>m</sub> and <b>H</b><sub>m</sub> are the matrices typical of the standard
        Krylov decomposition of <b>&Atilde;</b>:
      </p>
      <figure>
        <center>
          <img src="imgs/research_interests/exp-integrators/krylovdeco.png" width=100% height: auto>
        </center>
      </figure>
      <p>
        a prominent example of a Krylov-type routine is constituted by KIOPS from
        <a href="https://www.sciencedirect.com/science/article/pii/S0021999118304042">[GRT18]</a>.
        <br>
        The problem with this approach is that, when <b>A</b> is very large, the
        Arnoldi procedure employed to obtain the Krylov decomposition requires
        tons of memory operations and huge storage space (the matrix
        <b>V</b><sub>m</sub> gets really heavy for growing values of m).
        <br>
        To put things in perspective, Krylov-type methods in exponential
        integration call the Arnoldi procedure
      </p>
      <ul>
        <li>at each substep (a number in the tens of even hundreds)...</li>
        <li>...of each linear combination of phi-function (less than ten)...</li>
        <li>...of each exponential integration step (maybe hundreds, thousands, or even millions)</li>
      </ul>
      <p>
        amounting to an <b>insane</b> amount of calls to this <i>tiring</i>
        procedure.
        <br>
        Moreover, we noticed that, from a call to another to the Arnoldi
        decomposition, the setting does not change much: the matrix <b>&Atilde;</b>
        has a stucture as in figure
      </p>
      <figure>
      <center>
      <img src="imgs/research_interests/exp-integrators/AtildeAWJ.png" width=50% height: auto>
      <figcaption><small><small><p>
      To give an idea, usually <b>A</b> has millions of rows/columns while <b>J</b> is, like,
      a 4 by 4 matrix at most.
      </p></small></small></figcaption>
      </center>
      </figure>
      <p>
        In other words, exponential integration is a <b>vastly repetitive task</b>,
        and this makes us even more frustrated about all those Arnoldi calls.
      </p>
      <p>
        To tackle this issue we designed
        <a href="https://github.com/francozivcovich/bamphi">BAMPHI</a>,
        a routine for computing the action of the matrix exponential which is
        designed to collect and reuse the information about <b>A</b> gathered
        through the exponential integration steps.
        <br>
        To do so, we exploited the fact that the Krylov approximation above is
        mathematically equivalent to the polynomial approximation interpolating
        the exponential function at the Ritz’s values, i.e., the eigenvalues of
        <b>H</b><sub>m</sub>.
        Therefore, once the Ritz's values are computed once at the first go,
        BAMPHI continues the calculations by approximating the exponential
        function via a polynomial interpolation at the set of Ritz's values,
        bringing the overall number of calls to Arnoldi to one.

        On the other hand, BAMPHI almost always performs a larger number of matrix
        vector products.
        But the matrices of interests are not only very large but also
        <b>very sparse</b>, hence, all in all, BAMPHI manages to reach <b>unmatched
        levels of speed</b> on a variety of numerical experiments
        (see <a href="https://www.sciencedirect.com/science/article/pii/S0377042722005714">[CCZ23b]</a>).

        <br>

        As future work, we plan to make a comparison between Krylov-type methods
        and BAMPHI on a GPU architecture.
      </p>

      <p>
        <small>
        <i>References:</i>
        <br>
        [CCZ23b] M. Caliari, F. Cassini, F. Zivcovich, BAMPHI: Matrix-free and transpose-free action of linear combinations of &phi;-functions from exponential integrators, J. Comput. Appl. Math. 423 (2023) 114973
        <br>
        [GRT18]  S. Gaudreault, G. Rainwater, M. Tokman, KIOPS: A fast adaptive Krylov subspace solver for exponential integrators, J. Comput. Phys. 372 (2018) 236–255.
        </small>
      </p>




      <h4>Exponential integration and Shallow Water Equations</h4>

      <p>
        Atmospheric simulations are among the most prominent SWEs applications
        as the planar length scales are much greater than the vertical one
        (atmosphere wraps Earth as plastic wrap on a basketball).
        <br>
        Roughly speaking, due to the obvious symmetries at play, the basis
        functions used to for spatially discretize PDEs from SWE are usually
        smoother than the typical FE and with a larger intersection with other
        basis functions.
        This makes the discretization matrices usually less sparse, meaning that
        the matrix vector products are going to be more expensive, but also
        smaller, meaning that the Arnoldi procedure is less penalizing (see [GCDT22]).
      </p>

      <figure>
      <img src="imgs/research_interests/exp-integrators/swe-compressed.png" width=100% height: auto>
      <figcaption><small><small><p>
      In picture, a flattened Earth with some meteorological shenanigans going on.
      Credits for this <a href="https://www.youtube.com/watch?v=xf4PAjdtgZo">simulation</a>'s
      frame go to <a href="https://www.martin-schreiber.info/">Martin Schreiber</a>
      and his <a href="http://github.com/schreiberx/sweet" width=32% height: auto>SWEET</a> software.
      </p></small></small></figcaption>
      </figure>
      <p>
        Furthermore, the stiffest components in SWE systems use to describe the
        propagation of sound waves, that do not truly affect the simulation
        outcome (which is a fancy way to say that you won't make it rain by
        screaming at clouds).
        Therefore one would like to somehow ignore such components to factor out
        the huge computational cost connected to their evolution.
      </p>

      <p>
        What we are trying to do is to design a successful exponential-integrator/solutor
        coupling that takes into account the macro charactestics of this
        particular family of equations.
      </p>

      <p>
        <small>
        <i>References:</i>
        <br>
        [GCDT22] S. Gaudreault, M. Charron, V. Dallerit, M. Tokman, High-order numerical solutions to the shallow-water equations on the rotated cubed-sphere grid, J. Comput. Phys. 449 (2022) 110792
        </small>
      </p>



      <h4>Designing new numerical schemes for 'non-smooth' phenomena</h4>
      <p>
        In the hyperbolic setting, the pointwise smoothing typical of parabolic
        PDEs can not be expected. Rough or discontinuous initial data spread
        in the spatial and temporal domain breaking down "classical" integrators.
        Low-regularity exponential integrators are scheme that deeply embed the
        underlying structure of resonance into the numerical discretisation to
        PDEs allowing to prove powerful existence results for nonlinear PDEs at
        low regularity regimes.
      </p>

      <p>
        In
        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=it&user=oHN16zwAAAAJ&citation_for_view=oHN16zwAAAAJ:Y0pCki6q_DkC">[LSZ22]</a>
        we studied the numerical approximation of the semilinear Klein-Gordon
        equation
      </p>
      <figure>
        <center>
        <img src="imgs/research_interests/exp-integrators/kleingordon.png" width=70% height: auto>
        </figure>
        </center>
      </figure>
      <p>
        which, when f is the sine funciton, is often referred to as the sine-Gordon
        equation.
        This arises in many physical applications, such as magnetic-flux
        propagation in Josephson junctions, bloch-wall dynamics in magnetic
        crystals, propagation of dislocation in solid and liquid crystals,
        propagation of ultra-short optical pulses in two-level media.
      </p>
      <!-- <figure>
        <center>
          <img src="imgs/research_interests/exp-integrators/ultrashort.png" width=100% height: auto>
          <figcaption><small><small><p>
            Complete characterization of ultrashort optical pulses with a
            phase-shifting wedged reversal shearing interferometer Image credits
            to <a href="https://www.nature.com/articles/s41377-018-0022-0">"Light: Science & Applications"</a>.
          </p></small></small></figcaption>
        </center>
      </figure> -->
      <p>
        In particular, we discovered a cancellation structure that led us to
        derive a low-regularity correction of the Lie splitting method
      </p>
      <figure>
        <center>
        <img src="imgs/research_interests/exp-integrators/correctedLie.png" width=100% height: auto>
        </center>
      </figure>
      <p>
        where
      </p>
      <figure>
        <center>
        <img src="imgs/research_interests/exp-integrators/KGoperatorL.png" width=75% height: auto>
        </center>
      </figure>
      <p>
        This corrected scheme can have second-order convergence in the energy
        space under the regularity condition
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/exp-integrators/correctedLieregularitycondition.png" width=50% height: auto>
          </center>
        </figure>
      <p>
        where <i>d</i> = 1, 2, 3 denotes the dimension of space. In one dimension,
        the proposed method is shown to have a convergence order arbitrarily close
        to 5/3 in the energy space for solutions in the same space, i.e. no
        additional regularity in the solution is required.
      </p>
      <p>
        <small>
        <i>References:</i>
        <br>
        [LSZ22] B. Li, K. Schratz, F. Zivcovich, A second-order low-regularity correction of Lie splitting for the semilinear Klein-Gordon equation, ESAIM: M2AN, Forthcoming article (2022)
        <br>
        [RS21]  F. Rousset, K. Schratz: A general framework of low-regularity integrators. SIAM J. Numer. Anal. 59 (2021), pp. 1735–1768.
        </small>
      </p>



      <h3 id="optimal-control">Numerical methods for Multiscale and Optimal Control problems</h3>
      <p>
        Optimal Control problems for multiagent systems cover an important role
        in the applications.
        Examples amongst others include controlling storms of drones for
        monitoring large forests, evacuating crowds non-chaotically from large
        public structures such as airports, city centres, or malls.
        <br>
        <br>
        Multiagent systems problems are computationally hard to tackle as they
        require to evolve the behavior of N agents that constantly influence
        each other following intricate patterns.

        In particular, we started examining the problem of leading to consensus
        N agents, that is they reach the same velocity (vector), that operate
        under the interaction model
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/modelinteraction.png" width=90% height: auto>
          </center>
        </figure>
      <p>
        where P is some radial interaction kernel.
        Then we considered the following minimizing <b>non-differentiable
        control cost functional</b>:
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/minimizingfunctional.png" width=70% height: auto>
          </center>
        </figure>
      <p>
        where the scalars &nu; and &beta; tell how expensive is the control.
        Clearly, the consensus is penalized along both a quadratic control and
        a non-smooth, sparsity-promoting term (the 1-norm term).
        By doing this, we enforce sparsity on our optimal control strategy, with
        evident positive computational effects.
        Now, if we define the Hamiltonian as
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/hamiltonian492.png" width=90% height: auto>
          </center>
        </figure>
      <p>
        we can compute the following adjoint equations
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/hamiltonian493.png" width=80% height: auto>
          </center>
        </figure>
      <p>
        for every i = 0,1,...,N and final conditions p<sub>i</sub>(T)=0 and
        q<sub>i</sub>(T)=0.
        However,
        to get the optimality conditions we also need
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/hamiltonian494.png" width=45% height: auto>
          </center>
        </figure>
      <p>
        where D indicates the subdifferential of u at 0. To address this problem
        we had to resort to subdifferential theory. As a result, we found a
        componentwise relation between the optimal control u* and the Lagrange
        multipliers q appearing in the Hamiltonian:
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/uifofqi.png" width=40% height: auto>
          </center>
        </figure>
      <p>
        At this time, from an optimization point of view, deriving q still is
        particularly challenging as standard gradient-based numerical methods
        do not suit our non-smooth cost functional.
        To tackle this problem we had to consider a class of <b>iterative
        proximal gradient algorithms</b>, that are extensions of the classical
        gradient method.
        <br>
        <br>
        At this point, to complete our fast implementation, a bit of numerical
        craftiness was required: if we indicate with <b>P</b> the matrix whose
        (i,j) entry is P(||x<sub>i</sub>-x<sub>j</sub>||<sub>2</sub>) we can
        write in vector form
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/FASThamiltonian439-1.png" width=60% height: auto>
          </center>
        </figure>
      <p>
        where the dot indicates the componentwise product and 1 indicates the
        matrix of all ones of the specified dimension, then
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/FASThamiltonian439-2.png" width=42% height: auto>
          </center>
        </figure>
      <p>
        where we have
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/optimal-control/whoQ.png" width=95% height: auto>
          </center>
        </figure>
      <p>
        That is we singled out the matrices of our system of rank 1 and we
        exploited this to enhance performances.
        In fact, a rank one matrix <b>A</b> with N rows and N columns is a
        blessing, computationally speaking, as it can be written as the multiplication
        of a column vector and a row vector.
        This means that it can be stored in 2N space, the product between
        <b>A</b> and a vector b can be performed in O(N) operations.
        Also, the matrix multiplication between two rank-1 matrices can be done
        in O(N) and it produces, again, a rank-1 matrix.

        <br>
        <br>
        This, togheter with the extreme performances brought by the vectorization
        of the calculations exploiting BLAS routines, led to a highly performant
        software for simulating our multiagents Optimal Control system.
        <br>
        As a result, while the state-of-the-art code evoles and controls a
        system with N = 2000 agents in <b>83 hours</b>, our code does so in a
        <b>couple of minutes</b>.
        <br>
        <br>
        Thanks to these high performances we were able to study multiagent
        systems on an unprecedented scale (up to 10 thousands agents in two
        wall-clock hours).
      </p>
      <figure>
        <center>
          <img src="imgs/research_interests/optimal-control/uncontrolled.png" width=100% height: auto>
        <figcaption><small><small><p>
        Velocity of particles (x component vs. y component) over time in the uncontrolled case.
        <br>
        Number of agents N = 10<sup>4</sup>, time frames corresponding to t = 0, T/2,T, T = 5.
        </p></small></small></figcaption>
      </center>
      </figure>
      <figure>
        <center>
          <img src="imgs/research_interests/optimal-control/controlled.png" width=100% height: auto>
          <figcaption><small><small><p>
          Velocity of particles (x component vs. y component) over time in the only smooth control case.
          <br>
          Number of agents N = 10<sup>4</sup>, time frames corresponding to t = 0, T/2,T, T = 5.
          </p></small></small></figcaption>
      </center>
      </figure>
      <figure>
        <center>
          <img src="imgs/research_interests/optimal-control/controlledwithnonsmoothpenalization.png" width=100% height: auto>
          <figcaption><small><small><p>
          Velocity of particles (x component vs. y component) over time in the smooth and non-smooth control case.
          <br>
          Number of agents N = 10<sup>4</sup>, time frames corresponding to t = 0, T/2,T, T = 5.
          </p></small></small></figcaption>
      </center>
      </figure>

      <p>
        <small>
        <i>References:</i>
        <br>
        []
        </small>
      </p>

      <h3 id="num-lin-algebra">Numerical Linear Algebra and Arbitrary Precision arithmetic</h3>

      <p>
        Numerical Linear Algebra is the field where I come from.
      </p>


      <h4>Novel algorithm for computing the divided differences of analytic functions</h4>
      <p>
        Computing divided differences is a recursive division process.
        Given a sequence of points and a function f, the jth divided difference
        of f is given by
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/num-lin-alg-mul-mat-fun/standardrecurrence.png" width=100% height: auto>
          </center>
        </figure>
        <p>
        Unfortunately, this algorithm, called "standard recurrence", is a poor
        algorithm.
        Especially for confluent points, it suffers from large error propagation
        (due to all the <i>dividing</i>) and heavy catastrophic cancellation (due to all
        the <i>differencing</i>).

        I suppose, though, that in 17th century this was not felt as a big
        problem as only few divided differences were needed.
        Consider that, back then, these values were used to approximate logarithms,
        exponential, and trigonometric functions.
        Also, one could drag behind some operations down the sheet and, in case
        of cancellation, add some extra digits.

        <br>
        Then, in the Sixties, the diffusion of computers in the academies made
        scientists less adverse to algorithms with higher complexity.
        It was in this context that G. Opitz, in his 1964 paper [O64], shown a
        new way of computing the divided differences.
        The Opitz's theorem says that, given a function f, its divided differences
        can be computed as
      </p>
        <figure>
          <center>
            <img src="imgs/research_interests/num-lin-alg-mul-mat-fun/opitztheorem.png" width=80% height: auto>
          </center>
        </figure>
        <p>
          This algorithm is particularly powerful for it allows to compute the
          divided differences at points which are confluent or even coincident.
        <!-- <br>
          On the other hand, it requires the computation of a matrix function
          with very high relative precision.
          In fact, take for example the problem of computing the divided
          differences of the exponential function over 50 equispaced points
          between 0 and 1.
          The first column of the matrix exponential of the Opitz's matrix has
          entries with magnitude ranging from 1 to 1e-63.
          If a relative error criterion was to be used, say we want the relative
          error to be smaller than 1e-16, 19 exponential power series terms would
          do the job but every divided difference from the 20th to the last
          would be erroneously computed as 0.
          This is because the jth power of the Opitz's matrix has no more than
          j nonzero subdiagonals.
        <br>
          For this reason in 1984, McCurdy, Ng, and Parlett wrote a manuscript [MNP84]
          about an accurate way to form the divided differences of the exponential
          function through a modification of the Opitz's algorithm.
        <br> -->
        <br>
          In
          <a href="https://scholar.google.com/citations?view_op=view_citation&hl=it&user=oHN16zwAAAAJ&citation_for_view=oHN16zwAAAAJ:u5HHmVD_uO8C">[Z19]</a>
          I derived a new expansion of the divided differences that allowed to
          compute the divided differences with the same accuracy granted by the
          Opitz algorithm but with complexity O(n<sup>2</sup>) instead of
          O(n<sup>3</sup>).
          <br>
          <br>
          I am extra proud of this result since in 2020 it was used by a group
          of physicists for their Monte Carlo simulations of quantum many-body
          systems [GBH20] (also, they called it Zivcovich's algorithm &#128540;).


      </p>
      <p>
        <small>
        <i>References:</i>
        <br>
        [GBH20] L. Gupta, L. Barash, I. Hen, Calculating the divided differences of the exponential function by addition and removal of inputs, Comput. Phys. Commun., 254, (2020), 107385
        <!-- <br>
        [MNP84] A. McCurdy, K.C. Ng, B.N. Parlett, Accurate Computation of Divided Differences of the Exponential Function, Mathematics of Computation, Vol. 43, No. 168 (1984), pp. 501-528 -->
        <br>
        [O64] G. Opitz, Steigungsmatrizen, Z. Angew. Math. Mech. (1964), 44, T52–T54
        <br>
        [Z19] F. Zivcovich, Fast and accurate computation of divided differences for analytic functions, with an application to the exponential function, Dolomites Res. Notes Approx, 12, 28-42(2019)
        </small>
      </p>




      <h4>Arbitrary Precision computing of Functions of Matrices</h4>

      <p>
      Even though I explicitly said it is forbidden, computing the matrix
      exponential is something people sometimes do.
      Of course, the matrices they exponentiate are of modest sizes (less than 10
      thousands rows and columns, I would say) and rigorously dense.
      The reason for this is that the applications are many, we have seen
      a couple of them already in this research overview:
      </p>
      <ul>
        <li>exponentiation of the small matrices <b>A</b><sub>&mu;</sub> for the &mu;-mode products;</li>
        <li>exponentiation of the matrix <b>H</b><sub>m</sub> in Krylov-type methods.</li>
        <!-- <li>exponentiation of the Opitz's matrix for computing the divided differences of the exponential function.</li> -->
      </ul>
      <p>
        For the first application one needs speed, and the reason why is evident.
        For the second one, above all, one needs accuracy, most times without
        even knowing it.
        There are in fact examples where the exponentiation of <b>H</b><sub>m</sub>
        using routines able to reach a relative tolerance of 1e-16 (such as
        MATLAB's expm) return highly inaccurate results (check the example at the
        end of [CZ19, Section 6]).
        <br>
        <br>
        This is because in vectorial and matricial computations, reaching a
        relative accuracy equal to machine precision does not grant an accurate
        result.
        To understand this better, consider the problem of approximating the
        vector [M,0] with the vector [M,&epsilon;].
        The relative error in the norm induced by the scalar product equals
        &epsilon;M<sup>-1</sup>, which can <i>easily</i> get smaller than the machine
        precision.
        <br>
        <br>
        In [CZ19], we developed a routine, exptayotf, for computing the matrix
        exponential that reaches unmatched levels of speed.
        Also, it is <b>the sole routine</b> able to grant relative accuracies
        smaller than machine precision both in arbitrary precision arithmetic and
        in the standard single/double/half precisions.


      </p>
      <p>
        <small>
        <i>References:</i>
        <br>
        [AMH09] A.H. Al-Mohy, N.J. Higham, A new scaling and squaring algorithm for the matrix exponential, SIAM J. Matrix Anal. Appl. 31 (3) (2009) 970–989
        <br>
        [SIRD18] J.Sastre, J. Ibáñez, E.Defez, Boosting the computation of the matrix exponential, Appl. Math. Comput., Volume 340, 1 January 2019, Pages 206-220
        <br>
        [CZ19] M. Caliari, F. Zivcovich, On-the-fly backward error estimate for matrix exponential approximation by Taylor algorithm, J. Comput. Appl. Math., 346, 532-548, (2019)
        </small>
      </p>








      <h2 id="passion-projects">Passion projects</h2>
      <p>section under construction: hope to see you back in a while :)</p>

    </div>
  </div>



  <!-- <div class="navbar">
    <a href="#home" class="active">Home</a>
    <a href="#pubs">Publications</a>
    <a href="#software">Software </a>
    <a href="#more">More </a>
  </div> -->

</body>
</html>
